{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ab382e7b05f943538cc5472d5b75e49c","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"cd91f60fb5174f118356f4f232bb393c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"76df7a3963ef423787e24f30f0492e1c","deepnote_cell_type":"text-cell-h1"},"source":"# Training BERT Model for Final Project","block_group":"39132e27b6f543eb8deff07bea9cb58d"},{"cell_type":"code","metadata":{"source_hash":"2cf24369","execution_start":1746404593681,"execution_millis":3040,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"05f101400d2e48658fe1c71d82b1985b","deepnote_cell_type":"code"},"source":"import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd","block_group":"05f101400d2e48658fe1c71d82b1985b","execution_count":1,"outputs":[{"name":"stderr","text":"/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2025-05-05 00:23:14.721824: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-05-05 00:23:14.725249: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-05-05 00:23:14.756356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-05-05 00:23:14.756460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-05-05 00:23:14.757432: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-05 00:23:14.762631: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-05-05 00:23:14.763467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-05 00:23:15.631935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/e0ce2992-1a61-48e3-b262-31acaad697b4","content_dependencies":{"codeHash":"2cf24369","usedVariables":[],"importedModules":["torch","accuracy_score","nn","pd","DataLoader","os","get_linear_schedule_with_warmup","BertTokenizer","AdamW","Dataset","train_test_split","classification_report","BertModel"],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"a56bec3e","execution_start":1746404596771,"execution_millis":0,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"329f066bba5f4835bf725281059e0f31","deepnote_cell_type":"code"},"source":"def generate_samples(df, label, window=20, overlap=0):\n    all_words = []\n    samples = []\n\n    # Flatten all words from all lines into one list\n    for line in df['text']:\n        if isinstance(line, str):  # skip NaNs\n            all_words.extend(line.strip().split())\n\n    i = 0\n    # Create overlapping samples\n    while i + window <= len(all_words):\n        sample = all_words[i:i + window]\n        samples.append(' '.join(sample))\n        i += window - overlap\n\n    return pd.DataFrame({\n        'text': samples,\n        'label': [label] * len(samples)\n    })","block_group":"d822559d9f534386b3e1fd6c44eeeb64","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"a56bec3e","usedVariables":["window","line","samples","pd","sample","i","df","overlap","all_words","label"],"importedModules":[],"definedVariables":["generate_samples","line","samples","sample","i","all_words"]}},{"cell_type":"code","metadata":{"source_hash":"e5daa477","execution_start":1746404596821,"execution_millis":0,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"6ce7a7fca74e47668f9208ebc2624ed4","deepnote_cell_type":"code"},"source":"def load_data(shake_file, nonshake_file):\n    shakespeare_df = pd.read_csv(shake_file)\n    non_shakespeare_df = pd.read_csv(nonshake_file)\n\n    shakespeare_df = generate_samples(shakespeare_df, 1)\n    non_shakespeare_df = generate_samples(non_shakespeare_df, 0)\n\n    min_len = min(len(shakespeare_df), len(non_shakespeare_df))\n    shakespeare_df = shakespeare_df.sample(n=min_len, random_state=42).reset_index(drop=True)\n    non_shakespeare_df = non_shakespeare_df.sample(n=min_len, random_state=42).reset_index(drop=True)\n\n    combined_df = pd.concat([shakespeare_df, non_shakespeare_df], ignore_index=True)\n    combined_df = combined_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\n    return combined_df['text'].tolist(), combined_df['label'].tolist()","block_group":"660144dfa3674345b2833dd53488dc3d","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"e5daa477","usedVariables":["shake_file","shakespeare_df","nonshake_file","generate_samples","combined_df","pd","non_shakespeare_df","min_len"],"importedModules":[],"definedVariables":["shakespeare_df","combined_df","load_data","non_shakespeare_df","min_len"]}},{"cell_type":"code","metadata":{"source_hash":"77697a39","execution_start":1746404596871,"execution_millis":0,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"6ec459597a604ee18e6b4f6132e08383","deepnote_cell_type":"code"},"source":"class TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}","block_group":"ce3f35573c384d36a4535c783af70b1d","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"77697a39","usedVariables":["Dataset"],"importedModules":[],"definedVariables":["TextClassificationDataset"]}},{"cell_type":"code","metadata":{"source_hash":"c6c73903","execution_start":1746404596931,"execution_millis":0,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"13524158fcf842e6a362f43b9a133964","deepnote_cell_type":"code"},"source":"class BERTClassifier(nn.Module):\n    def __init__(self, bert_model_name, num_classes):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        x = self.dropout(pooled_output)\n        logits = self.fc(x)\n        return logits\n\ndef train(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    for batch in data_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    \ndef evaluate(model, data_loader, device):\n    model.eval()\n    predictions = []\n    actual_labels = []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().tolist())\n            actual_labels.extend(labels.cpu().tolist())\n    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n\ndef predict_sentiment(text, model, tokenizer, device, max_length=128):\n    model.eval()\n    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n    return \"Shakespeare\" if preds.item() == 1 else \"Non-Shakespeare\"","block_group":"2ed0f689e59742c5a01e3050cbe6a459","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":{"error":{"type":"AttributeError","message":"'Attribute' object has no attribute 'id'"},"codeHash":"c6c73903","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"c05de83","execution_start":1746404596981,"execution_millis":5998,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"30669e2bbf76463c952eff6221d02bb7","deepnote_cell_type":"code"},"source":"bert_model_name = 'bert-base-uncased'\nnum_classes = 2\nmax_length = 128\nbatch_size = 16\nnum_epochs = 4\nlearning_rate = 2e-5\n\ntexts, labels = load_data('shakespeare.csv', 'nonShakespeare.csv')\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name)\ntrain_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\nval_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\ndevice = \"cpu\"\nmodel = BERTClassifier(bert_model_name, num_classes).to(device)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(train_dataloader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","block_group":"12b0a6deed194a558d012300bcf74644","execution_count":6,"outputs":[{"name":"stderr","text":"Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 41.9MB/s]\nDownloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 353kB/s]\nDownloading config.json: 100%|██████████| 570/570 [00:00<00:00, 4.40MB/s]\nDownloading pytorch_model.bin: 100%|██████████| 420M/420M [00:04<00:00, 108MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/root/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/512047ce-5d8f-45fd-9549-cefdf70f8d87","content_dependencies":{"codeHash":"c05de83","usedVariables":["val_dataset","train_texts","tokenizer","model","optimizer","max_length","num_epochs","BertTokenizer","TextClassificationDataset","BERTClassifier","DataLoader","AdamW","train_test_split","bert_model_name","get_linear_schedule_with_warmup","total_steps","num_classes","batch_size","val_labels","train_dataset","labels","device","train_dataloader","val_texts","texts","learning_rate","load_data","train_labels"],"importedModules":[],"definedVariables":["val_dataset","train_texts","tokenizer","model","optimizer","val_dataloader","max_length","num_epochs","scheduler","bert_model_name","total_steps","num_classes","batch_size","val_labels","train_dataset","labels","device","train_dataloader","val_texts","texts","learning_rate","train_labels"]}},{"cell_type":"code","metadata":{"source_hash":"409d9434","execution_start":1746404603031,"execution_millis":289900,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"463f1a11837e4532a8fd1cc85abbb2e4","deepnote_cell_type":"code"},"source":"for epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    train(model, train_dataloader, optimizer, scheduler, device)\n    accuracy, report = evaluate(model, val_dataloader, device)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(report)\n\ntorch.save(model.state_dict(), \"bert_classifier.pth\")","block_group":"3c46f4198897477cbbab5087e084f7bd","execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/4\nValidation Accuracy: 0.8636\n              precision    recall  f1-score   support\n\n           0       0.80      1.00      0.89        37\n           1       1.00      0.69      0.82        29\n\n    accuracy                           0.86        66\n   macro avg       0.90      0.84      0.85        66\nweighted avg       0.89      0.86      0.86        66\n\nEpoch 2/4\nValidation Accuracy: 0.9545\n              precision    recall  f1-score   support\n\n           0       1.00      0.92      0.96        37\n           1       0.91      1.00      0.95        29\n\n    accuracy                           0.95        66\n   macro avg       0.95      0.96      0.95        66\nweighted avg       0.96      0.95      0.95        66\n\nEpoch 3/4\nValidation Accuracy: 0.9545\n              precision    recall  f1-score   support\n\n           0       1.00      0.92      0.96        37\n           1       0.91      1.00      0.95        29\n\n    accuracy                           0.95        66\n   macro avg       0.95      0.96      0.95        66\nweighted avg       0.96      0.95      0.95        66\n\nEpoch 4/4\nValidation Accuracy: 0.9697\n              precision    recall  f1-score   support\n\n           0       1.00      0.95      0.97        37\n           1       0.94      1.00      0.97        29\n\n    accuracy                           0.97        66\n   macro avg       0.97      0.97      0.97        66\nweighted avg       0.97      0.97      0.97        66\n\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/81ed8672-7ad0-4822-aaf5-ab8cddb8b6c4","content_dependencies":{"codeHash":"409d9434","usedVariables":["accuracy","train","report","num_epochs","device","torch","evaluate","val_dataloader","train_dataloader","optimizer","model","scheduler","epoch"],"importedModules":[],"definedVariables":["accuracy","epoch","report"]}},{"cell_type":"code","metadata":{"source_hash":"577003a5","execution_start":1746404985029,"execution_millis":101,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"273f3c6e05b8483b8985cca0cc2055ed","deepnote_cell_type":"code"},"source":"test_text = \"life maintains Theridamas. TAMBURLAINE. Theridamas, my friend, take here my hand, Which is as much as if I swore by\"\nsentiment = predict_sentiment(test_text, model, tokenizer, device)\nprint(\"life maintains Theridamas. TAMBURLAINE. Theridamas, my friend, take here my hand, Which is as much as if I swore by\")\nprint(f\"Predicted sentiment: {sentiment}\")","block_group":"c7ffbfe5ea084f27b15a7defde27cbda","execution_count":14,"outputs":[{"name":"stdout","text":"life maintains Theridamas. TAMBURLAINE. Theridamas, my friend, take here my hand, Which is as much as if I swore by\nPredicted sentiment: Non-Shakespeare\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/8d9ef4f2-bf7e-4848-a9ae-a1dd6a15d18a","content_dependencies":{"codeHash":"577003a5","usedVariables":["predict_sentiment","test_text","model","tokenizer","device","sentiment"],"importedModules":[],"definedVariables":["test_text","sentiment"]}},{"cell_type":"code","metadata":{"source_hash":"9db40e2c","execution_start":1746405192791,"execution_millis":1126,"execution_context_id":"64d7f21c-c76c-47ca-a561-8e1659a3dde5","cell_id":"0c0c4ef0be364e1f827969fc94194a73","deepnote_cell_type":"code"},"source":"for i in range(10, 20):\n    sentiment = predict_sentiment(val_texts[i], model, tokenizer, device)\n    print(val_texts[i], val_labels[i])\n    print(f\"Predicted sentiment: {sentiment}\")","block_group":"a5280793c6d342bf816e6663860ffd02","execution_count":23,"outputs":[{"name":"stdout","text":"half serve me. FACE. No, sir! buy The covering off o' churches. MAM. That's true. FACE. Yes. Let them stand 0\nPredicted sentiment: Non-Shakespeare\nGo in and see, you traitor. Go! [EXIT FACE.] MAM. Who is it, sir? SUB. Nothing, sir; nothing. MAM. What's 0\nPredicted sentiment: Non-Shakespeare\nwas not this nigh shore? ARIEL: Close by PROSPERO: But are they ARIEL: Not a hair perish'd; On their sustaining 1\nPredicted sentiment: Shakespeare\nDoth it not then our eyelids sink? I find not Myself disposed to sleep. ANTONIO: Nor I; my spirits are 1\nPredicted sentiment: Shakespeare\nworks: Beside, we should give somewhat to man's nature, The place he lives in, still about the fire, And fume 0\nPredicted sentiment: Shakespeare\nand carbuncle. My foot-boy shall eat pheasants, calver'd salmons, Knots, godwits, lampreys: I myself will have The beards of barbels 0\nPredicted sentiment: Non-Shakespeare\nmajesty complain Of Tamburlaine, that sturdy Scythian thief, That robs your merchants of Persepolis Trading by land unto the Western 0\nPredicted sentiment: Non-Shakespeare\nWhy, rascal! FACE. Lo you!--Here, sir! [EXIT.] MAM. 'Fore God, a Bradamante, a brave piece. SUR. Heart, this is a 0\nPredicted sentiment: Non-Shakespeare\nparents Sincere professors? SUB. Why do you ask? ANA. Because We then are to deal justly, and give, in truth, 0\nPredicted sentiment: Non-Shakespeare\nand exalted thee, and fix'd thee In the third region, call'd our state of grace? Wrought thee to spirit, to 0\nPredicted sentiment: Non-Shakespeare\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/872ffb89-aa9f-45ba-ac47-2a31bc718f1a","content_dependencies":{"codeHash":"9db40e2c","usedVariables":["predict_sentiment","i","model","val_labels","val_texts","tokenizer","device","sentiment"],"importedModules":[],"definedVariables":["i","sentiment"]}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4cd1bb92-5c33-4c71-9500-71d9821d29e2' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"01f94b7c5e6d4308aa0ee30b0e12725b"}}